{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhCnOZ3Ur98sq3WGHsd8Uh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krithikajain/bart_model/blob/main/bart_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-XV5pzvaoDB",
        "outputId": "7b6dc81a-0edb-46ba-e29c-00dd047e4fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=8203e92fa28cdffbb501f097b2469df905149d48a0fb769b5902fe60517adb1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUSYtN5Da7jM",
        "outputId": "e623ea12-04cb-43d7-aa74-4af224d6646f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmX5iRVEaXrj",
        "outputId": "77417e4e-065a-40ae-e4a0-da2791fbf2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary The case was brought by a widow and her daughter who had married a man who owned a number of properties in the tirunelveli district. The husband of the widow had left a will to his wife and children. The widow sold one of the properties to the 2nd defendant for a sum of £500. The appeal was granted and the supreme court was granted the appeal to the supreme Court of India.\n",
            "{'rouge1': Score(precision=0.0445906432748538, recall=0.8356164383561644, fmeasure=0.08466342817487854), 'rouge2': Score(precision=0.023408924652523776, recall=0.4444444444444444, fmeasure=0.04447533009034051), 'rougeL': Score(precision=0.03216374269005848, recall=0.6027397260273972, fmeasure=0.061068702290076333)}\n"
          ]
        }
      ],
      "source": [
        "import transformers \n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig \n",
        "import nltk \n",
        "nltk.download('punkt') \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import sent_tokenize,word_tokenize \n",
        "from nltk.stem import PorterStemmer \n",
        "import string \n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "nltk.download('stopwords') \n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "extracted_text=' ' \n",
        "with open('/content/drive/MyDrive/legal_data/judgement/1953_L_1.txt', 'r') as file: \n",
        "      extracted_text = file.read()\n",
        "      \n",
        "#stemming tokens \n",
        "\n",
        "#function for tokenization and removing stopwords\n",
        "def remove_stop_words(text):\n",
        "    punct_removed_text = text.translate(str.maketrans('','',string.punctuation))\n",
        "    words = nltk.word_tokenize(punct_removed_text)\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "#function for stemming\n",
        "def stemming(text):\n",
        "    tokens = text.split(' ')\n",
        "\n",
        "    #defining a Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    #stem the tokens \n",
        "    stemmed_tokens = []\n",
        "\n",
        "\n",
        "    for token in tokens:\n",
        "        stemmed_token = stemmer.stem(token)\n",
        "        stemmed_tokens.append(stemmed_token)\n",
        "\n",
        "    #join the stemmed tokens back into a string\n",
        "    stemmed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "text_without_stopwords = remove_stop_words(extracted_text) #removing stopwords (function called)\n",
        "stem_text = stemming(text_without_stopwords)\n",
        "sentences = sent_tokenize(extracted_text)\n",
        "text = \"summarize:\" + stem_text\n",
        "\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "    #BART MODEL ------------\n",
        "\n",
        "reference_summary=' '\n",
        "with open('/content/drive/MyDrive/legal_data/summary/full/A1/1953_L_1.txt', 'r') as file:\n",
        "    reference_summary = file.read()\n",
        "\n",
        "bart_inputs = bart_tokenizer.encode(extracted_text,return_tensors='pt', max_length=1024, truncation=True)\n",
        "bart_summary_ids = bart_model.generate(bart_inputs, num_beams=1, max_length=400, early_stopping=True)\n",
        "#     bart_summary = bart_tokenizer.decode(bart_summary_ids[0],skip_special_tokens=True)\n",
        "bart_summary = bart_tokenizer.decode(bart_summary_ids[0],skip_special_tokens=True)\n",
        "print('summary',bart_summary)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(str(bart_summary), reference_summary)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The case was brought by a widow and her daughter who had married a man who owned a number of properties in the tirunelveli district. The husband of the widow had left a will to his wife and children. The widow sold one of the properties to the 2nd defendant for a sum of £500. The appeal was granted and the supreme court was granted the appeal to the supreme Court of India."
      ],
      "metadata": {
        "id": "rXncU3MbGqKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers \n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig \n",
        "import nltk \n",
        "nltk.download('punkt') \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import sent_tokenize,word_tokenize \n",
        "from nltk.stem import PorterStemmer \n",
        "import string \n",
        "\n",
        "nltk.download('stopwords') \n",
        "stop_words = set(stopwords.words('english'))\n",
        "extracted_text=' ' \n",
        "with open('/content/drive/MyDrive/legal_data/judgement/2015_J_10.txt', 'r') as file: \n",
        "      extracted_text = file.read()\n",
        "    \n",
        "sentences = sent_tokenize(extracted_text)\n",
        "\n",
        "reference_summary=' '\n",
        "with open('/content/drive/MyDrive/legal_data/summary/segment-wise/A2/facts/2015_J_10.txt', 'r') as file:\n",
        "    reference_summary = file.read()\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "bart_inputs = bart_tokenizer.encode(extracted_text,return_tensors='pt', max_length=1024, truncation=True)\n",
        "bart_summary_ids = bart_model.generate(bart_inputs, num_beams=4, max_length=400, early_stopping=True)\n",
        "bart_summary = bart_tokenizer.decode(bart_summary_ids[0],skip_special_tokens=True) \n",
        "print('bart summary',bart_summary)\n",
        "# print('reference summary',reference_summary)\n",
        "# Calculate ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(str(bart_summary), reference_summary)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7i5A2iRDxLp",
        "outputId": "81532a8d-a216-4d3b-b43c-1ed3b01841f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bart summary This appeal is preferred against the judgment dated 19 8 2011 passed by the high court of punjab and haryana in criminal appeal no 181 sb of 2000. The high court partly allowed the appeal filed by the appellants thereby confirming the conviction of the appellant with certain modifications. The accused persons were convicted of trespassing into the land belonging to the accused and attempted to forcibly cut the pullas.\n",
            "{'rouge1': Score(precision=0.09429280397022333, recall=0.5428571428571428, fmeasure=0.160676532769556), 'rouge2': Score(precision=0.03980099502487562, recall=0.2318840579710145, fmeasure=0.06794055201698514), 'rougeL': Score(precision=0.08188585607940446, recall=0.4714285714285714, fmeasure=0.13953488372093023)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bart summary This appeal is preferred against the judgment dated 19 8 2011 passed by the high court of punjab and haryana in criminal appeal no 181 sb of 2000. The high court partly allowed the appeal filed by the appellants thereby confirming the conviction of the appellant with certain modifications. The accused persons were convicted of trespassing into the land belonging to the accused and attempted to forcibly cut the pullas.\n"
      ],
      "metadata": {
        "id": "q9PAiaDi0Pl-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYtPm0Ljye5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}